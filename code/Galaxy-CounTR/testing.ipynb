{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"snVQxdymXfT1"},"outputs":[],"source":["#@title Install dependencies\n","!pip install -q torch torchvision torchaudio-f https://download.pytorch.org/whl/torch_stable.html\n","!pip install -q timm==0.4.12\n","!pip install -q tensorboard\n","!pip3 install -q hub\n","!pip install -q pytorch_lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"MAhy1VzYX_lz"},"outputs":[],"source":["#@title Imports\n","import json\n","import numpy as np\n","import os\n","import math\n","import time\n","import random\n","import sys\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import timm\n","import torch\n","from torchvision import transforms\n","import albumentations as A\n","import cv2\n","import pytorch_lightning as pl\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision\n","from collections import OrderedDict\n","import timm.optim.optim_factory as optim_factory\n","import util.misc as misc\n","import models_mae_cross\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from util.misc import NativeScalerWithGradNormCount as NativeScaler\n","\n","if not torch.cuda.is_available():\n","    device=torch.device(\"cpu\")\n","    print(\"Current device:\", device)\n","else:\n","    device=torch.device(\"cuda\")\n","    print(\"Current device:\", device, \"- Type:\", torch.cuda.get_device_name(0))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"QgrYMx7WYHWu"},"outputs":[],"source":["#@title Load Best Checkpoint\n","paper = False #@param {type:\"boolean\"}\n","PATH = 'countrV6' #@param {type:\"string\"}\n","DATA_PATH = '../data/GalaxiesV2/' #@param {type:\"string\"}\n","ANNO_FILE = 'annotation.json' #@param {type:\"string\"}\n","DATA_SPLIT_FILE = 'train_test_val.json' #@param {type:\"string\"}\n","IM_DIR = 'images/' #@param {type:\"string\"}\n","GT_DIR = 'density_maps/' #@param {type:\"string\"}\n","\n","anno_file = DATA_PATH + ANNO_FILE\n","data_split_file = DATA_PATH + DATA_SPLIT_FILE\n","im_dir = DATA_PATH + IM_DIR\n","gt_dir = DATA_PATH + GT_DIR\n","\n","with open(anno_file) as f:\n","    annotations = json.load(f)\n","\n","with open(data_split_file) as f:\n","    data_split = json.load(f)\n","\n","# different loading pipeline between paper and lightning training \n","if paper:\n","  PATH_TO_CKPT = f'./checkpoints/{path}.pth'\n","  args = torch.load(PATH_TO_CKPT)['args']\n","  args.resume = PATH_TO_CKPT\n","  model = models_mae_cross.__dict__['mae_vit_base_patch16'](norm_pix_loss=False)\n","  misc.load_model_FSC(args=args, model_without_ddp=model)\n","\n","else:\n","  PATH_TO_CKPT = f'./checkpoints/{PATH}.ckpt'\n","  model = models_mae_cross.__dict__['mae_vit_base_patch16'](norm_pix_loss=False)\n","  state_dict = torch.load(PATH_TO_CKPT, map_location=torch.device('cpu'))['state_dict']\n","  pl_state_dict = OrderedDict([(key[6:], state_dict[key]) for key in state_dict.keys()])\n","  model.load_state_dict(pl_state_dict)\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mDslZzvSYZbI"},"outputs":[],"source":["#@title Modified Class Dataset for Testing\n","class StarsDataset(Dataset):\n","    def __init__(self, split, plot=False, transform=None):    \n","\n","        # added a plot parameter to consider only 5 samples when loading data to plot    \n","        self.img = data_split[split][:5] if plot==True else data_split[split]\n","        self.img_dir = im_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img)\n","\n","    def __getitem__(self, idx):\n","        im_id = self.img[idx]\n","        anno = annotations[im_id]\n","        bboxes = anno['box_examples_coordinates']\n","\n","        rects = list()\n","        for bbox in bboxes:\n","            x1 = bbox[0][0]\n","            y1 = bbox[0][1]\n","            x2 = bbox[2][0]\n","            y2 = bbox[2][1]\n","            rects.append([y1, x1, y2, x2])\n","\n","        dots = np.array(anno['points'])\n","        image = np.array(Image.open(im_dir+im_id))\n","        density = np.load(gt_dir+im_id[:-4] + '.npy').astype('float32')   \n","        m_flag = 0\n","\n","        boxes = list()\n","        for box in rects:\n","            y1, x1, y2, x2 = [int(k) for k in box]  \n","            bbox = Image.fromarray(image[y1:y2+1, x1:x2+1, :])\n","            bbox = transforms.Resize((64, 64))(bbox)\n","            boxes.append(transforms.ToTensor()(bbox))\n","        boxes = torch.stack(boxes)\n","\n","        if self.transform!=None:\n","            aug = self.transform(image=image, mask=density)\n","            image = aug['image']\n","            density = aug['mask']\n","        \n","        # boxes shape [3,3,64,64], image shape [3,384,384], density shape[384,384]   \n","        norm = A.Normalize()(image = image, mask = density)\n","        sample = {'image':norm['image'].transpose(2, 0, 1), 'dots':dots.shape[0], 'boxes':boxes, 'pos':rects, 'gt_map':density}\n","\n","        return sample['image'], sample['dots'], sample['boxes'], sample['pos'], sample['gt_map']"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"RZuzWX7J9T7Q"},"outputs":[],"source":["#@title MAE Class\n","class customMAE(nn.Module):\n","    def __init__(self):\n","       super().__init__()\n","       self.mae = nn.L1Loss()\n","\n","    def forward(self, yhat, y):       \n","        pred_cnt = torch.sum(yhat/factor, dim=(1,2))\n","        return self.mae(pred_cnt, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"vLxV1KBzbuGU"},"outputs":[],"source":["#@title RMSE Class\n","class customRMSE(nn.Module):\n","    def __init__(self):\n","       super().__init__()\n","       self.mse = nn.MSELoss()\n","\n","    def forward(self, yhat, y):       \n","        pred_cnt = torch.sum(yhat/factor, dim=(1,2))\n","        return math.sqrt(self.mse(pred_cnt, y))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"LprWVofdAVc6"},"outputs":[],"source":["#@title Plot Predictions Function\n","def plot_predictions(fig, gt_map, output, path, batch_id):\n","\n","    plt.figure(figsize = (30, 20));\n","    plt.subplot(1, 3, 1);\n","    plt.axis('off');\n","    plt.title('Input Image', fontsize=20);\n","    plt.imshow(fig.detach().cpu().numpy().transpose(1,2,0));\n","\n","    plt.subplot(1, 3, 2);\n","    plt.axis('off');\n","    plt.title(f'Groundtruth Density = {int(torch.sum(gt_map/factor, dim=(1,2)))}', fontsize=20);\n","    plt.imshow(gt_map[0].detach().cpu(), cmap='gray');\n","\n","    plt.subplot(1, 3, 3);\n","    plt.axis('off');\n","    plt.title(f'Predicted Density = {round(torch.sum(output/factor, dim=(1,2)).item(), 1)}', fontsize=20);\n","    plt.imshow(output.squeeze(0).detach().cpu(), cmap='hot');\n","\n","    plt.savefig(f'../test/{path}/plots/{path}_{batch_id}')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1671648441179,"user":{"displayName":"arturo ghinassi","userId":"02751271912569120406"},"user_tz":-60},"id":"FEKFQHfDZFsT","outputId":"bee2303b-775d-41de-a7c7-7a1c41b674f0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of test samples: 153\n"]}],"source":["#@title Test Dataloader\n","batch_size = 1\n","test_dataset = StarsDataset('test')\n","test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","mae = customMAE()\n","rmse = customRMSE()\n","print('Number of test samples:', len(test_dl))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_E0rLBySb43s"},"outputs":[],"source":["#@title Testing\n","model.eval()\n","test_MAE = 0\n","test_RMSE = 0\n","\n","spread = True #@param {type:\"boolean\", description:\"if True factor is 60\"}\n","factor = 60 if spread else 0\n","\n","shots = 'zero' #@param {type:\"string\", options:\"either zero or few, if few automatic set to 3\"}\n","num_ex = 0 if shots=='zero' else 3\n","\n","if not os.path.isdir(f'../test/{PATH}'):\n","  os.mkdir(f'../test/{PATH}')\n","  os.mkdir(f'../test/{PATH}/few')\n","  os.mkdir(f'../test/{PATH}/zero')\n","\n","with torch.no_grad():\n","  for batch_id, batch in enumerate(test_dl):  \n","      samples, gt_dots, boxes, pos, gt_map = batch\n","      samples = samples.to(device)\n","      gt_dots = gt_dots.to(device)\n","      boxes = boxes.to(device)\n","      gt_map = gt_map.to(device)\n","\n","      output = model(samples, boxes, num_ex)\n","      test_MAE += mae(output, gt_dots)\n","      test_RMSE += rmse(output, gt_dots)\n","\n","      fig = samples[0]\n","      box_map = torch.zeros([fig.shape[1],fig.shape[2]])\n","      box_map = box_map.to(device, non_blocking=True)\n","      for rect in pos:      \n","          for i in range(rect[2]-rect[0]):\n","              box_map[min(rect[0]+i,fig.shape[1]-1),min(rect[1],fig.shape[2]-1)] = 10\n","              box_map[min(rect[0]+i,fig.shape[1]-1),min(rect[3],fig.shape[2]-1)] = 10\n","          for i in range(rect[3]-rect[1]):\n","              box_map[min(rect[0],fig.shape[1]-1),min(rect[1]+i,fig.shape[2]-1)] = 10\n","              box_map[min(rect[2],fig.shape[1]-1),min(rect[1]+i,fig.shape[2]-1)] = 10\n","      box_map = box_map.unsqueeze(0).repeat(3,1,1) \n","      pred = output.repeat(3,1,1)\n","      fig = fig + box_map + pred/2\n","      fig = torch.clamp(fig, 0, 1)\n","      plot_predictions(fig, gt_map, output, PATH, batch_id)\n","\n","with open(f'../test/{PATH}/{shots}/metrics.txt', 'w') as f:\n","     f.write(f'Test MAE: {(test_MAE/len(test_dl)).item()}\\nTest RMSE: {(test_RMSE/len(test_dl))}')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
